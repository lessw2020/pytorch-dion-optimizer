{
  "dion": [
    {
      "step": 0,
      "loss": 10.892661094665527,
      "learning_rate": 0.01,
      "grad_norm": 0.6727918741796823,
      "wall_time": 0.36500048637390137,
      "memory_used": 1211.69580078125
    },
    {
      "step": 10,
      "loss": 9.969836235046387,
      "learning_rate": 0.01,
      "grad_norm": 0.6331134372612196,
      "wall_time": 0.7001371383666992,
      "memory_used": 1211.69580078125
    },
    {
      "step": 20,
      "loss": 8.270310401916504,
      "learning_rate": 0.01,
      "grad_norm": 0.6101815033117788,
      "wall_time": 1.040113925933838,
      "memory_used": 1211.69580078125
    },
    {
      "step": 30,
      "loss": 6.297362327575684,
      "learning_rate": 0.01,
      "grad_norm": 0.5791787797965559,
      "wall_time": 1.379974126815796,
      "memory_used": 1211.69580078125
    },
    {
      "step": 40,
      "loss": 4.56975793838501,
      "learning_rate": 0.01,
      "grad_norm": 0.49948540595634816,
      "wall_time": 1.719963550567627,
      "memory_used": 1211.69580078125
    },
    {
      "step": 50,
      "loss": 3.5285046100616455,
      "learning_rate": 0.01,
      "grad_norm": 0.4480876218548933,
      "wall_time": 2.060314178466797,
      "memory_used": 1211.69580078125
    },
    {
      "step": 60,
      "loss": 3.012704372406006,
      "learning_rate": 0.01,
      "grad_norm": 0.43653437403115997,
      "wall_time": 2.399643898010254,
      "memory_used": 1211.69580078125
    },
    {
      "step": 70,
      "loss": 2.6529672145843506,
      "learning_rate": 0.01,
      "grad_norm": 0.4418726492445018,
      "wall_time": 2.738633155822754,
      "memory_used": 1211.69580078125
    },
    {
      "step": 80,
      "loss": 2.4553401470184326,
      "learning_rate": 0.01,
      "grad_norm": 0.4159845274321563,
      "wall_time": 3.07771897315979,
      "memory_used": 1211.69580078125
    },
    {
      "step": 90,
      "loss": 2.2968575954437256,
      "learning_rate": 0.01,
      "grad_norm": 0.3762162541326182,
      "wall_time": 3.4165594577789307,
      "memory_used": 1211.69580078125
    },
    {
      "step": 99,
      "loss": 2.2768120765686035,
      "learning_rate": 0.01,
      "grad_norm": 0.3818223699819877,
      "wall_time": 3.7219159603118896,
      "memory_used": 1211.69580078125
    }
  ],
  "adamw": [
    {
      "step": 0,
      "loss": 10.880025863647461,
      "learning_rate": 0.0002,
      "grad_norm": 0.6752852010691294,
      "wall_time": 0.02628469467163086,
      "memory_used": 1741.39111328125
    },
    {
      "step": 10,
      "loss": 10.540284156799316,
      "learning_rate": 0.0002,
      "grad_norm": 0.4794742810091439,
      "wall_time": 0.23054099082946777,
      "memory_used": 1740.51611328125
    },
    {
      "step": 20,
      "loss": 10.131644248962402,
      "learning_rate": 0.0002,
      "grad_norm": 0.42851434176663233,
      "wall_time": 0.4328787326812744,
      "memory_used": 1740.51611328125
    },
    {
      "step": 30,
      "loss": 9.784619331359863,
      "learning_rate": 0.0002,
      "grad_norm": 0.45463112813024475,
      "wall_time": 0.6344432830810547,
      "memory_used": 1740.51611328125
    },
    {
      "step": 40,
      "loss": 9.486592292785645,
      "learning_rate": 0.0002,
      "grad_norm": 0.45922491127720183,
      "wall_time": 0.8362774848937988,
      "memory_used": 1740.51611328125
    },
    {
      "step": 50,
      "loss": 9.21857738494873,
      "learning_rate": 0.0002,
      "grad_norm": 0.48935387498453414,
      "wall_time": 1.037888765335083,
      "memory_used": 1740.51611328125
    },
    {
      "step": 60,
      "loss": 8.96893310546875,
      "learning_rate": 0.0002,
      "grad_norm": 0.47948324162543243,
      "wall_time": 1.2396790981292725,
      "memory_used": 1740.51611328125
    },
    {
      "step": 70,
      "loss": 8.711746215820312,
      "learning_rate": 0.0002,
      "grad_norm": 0.49693305908908864,
      "wall_time": 1.4413158893585205,
      "memory_used": 1740.51611328125
    },
    {
      "step": 80,
      "loss": 8.45991325378418,
      "learning_rate": 0.0002,
      "grad_norm": 0.5001021652647561,
      "wall_time": 1.6433813571929932,
      "memory_used": 1740.51611328125
    },
    {
      "step": 90,
      "loss": 8.20776081085205,
      "learning_rate": 0.0002,
      "grad_norm": 0.4911242000348289,
      "wall_time": 1.8451588153839111,
      "memory_used": 1740.51611328125
    },
    {
      "step": 99,
      "loss": 7.982591152191162,
      "learning_rate": 0.0002,
      "grad_norm": 0.491348542479941,
      "wall_time": 2.027020215988159,
      "memory_used": 1740.51611328125
    }
  ]
}